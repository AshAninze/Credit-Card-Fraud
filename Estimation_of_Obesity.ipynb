{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "170ivR0Rz1QoLSj3axjTWmQBQ1jRQlzXx",
      "authorship_tag": "ABX9TyNEw8+NDlowHbgPz5NAsiyJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AshAninze/Uni-Projects/blob/main/Estimation_of_Obesity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "This code implements a machine learning pipeline for predicting obesity levels. It includes data preprocessing, feature engineering, model training with hyperparameter tuning, and evaluation using cross-validation. The pipeline explores multiple models, including SVC, Random Forest, Logistic Regression, and Decision Tree, to identify the best-performing model for this classification task."
      ],
      "metadata": {
        "id": "HE5sVG5gawIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing all needed packages for ml pipeline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import cross_val_score, train_test_split, KFold, cross_val_predict\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from timeit import default_timer as timer\n",
        "from sklearn.svm import SVC\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")  # ignoring any warnings\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import uniform, randint\n",
        "import os\n",
        "current_directory = os.getcwd()  # gets the current directory to be able to find the data file\n",
        "\n",
        "\n",
        "class DataSciencePipeline:\n",
        "    \"\"\" A class to create multiple machine learning models on a given dataset\n",
        "\n",
        "      This class encapsulates the entire data science pipeline, including:\n",
        "      - Data Loading\n",
        "      - Data preprocessing\n",
        "      - Feature Engineering\n",
        "      - Data splitting\n",
        "      - Hyperparameter tuning with cross validation\n",
        "      - Model training and evaluation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\" Initializes the DataSciencePipeline with the given dataset.\n",
        "        Attributes:\n",
        "          data: The pandas DataFrame holding the dataset.\n",
        "          X: Features (independent variables) after preprocessing.\n",
        "          y: Target variable (dependent variable).\n",
        "          no_X: Features without scaling, used for comparison.\n",
        "          no_y: Target variable without scaling.\n",
        "          X_train: Training features variables.\n",
        "          X_test: Testing features variables.\n",
        "          y_train: Training target variable.\n",
        "          y_test: Testing target variable.\n",
        "        \"\"\"\n",
        "        self.data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Postgrad/Comp4AI/ObesityDataSet_raw_and_data_sinthetic.csv')\n",
        "        self.X = None\n",
        "        self.y = None\n",
        "        self.no_X = None\n",
        "        self.no_y = None\n",
        "        self.X_train = None\n",
        "        self.X_test = None\n",
        "        self.y_train = None\n",
        "        self.y_test = None\n",
        "\n",
        "    def preprocess_data(self, categorical_columns, numerical_columns):\n",
        "        \"\"\"\n",
        "        Preprocesses the data by rounding any numerical values to the nearest integer\n",
        "        and then performing label encoding on categorical features.\n",
        "\n",
        "        Attributes:\n",
        "          data['Height']: Height in m\n",
        "          data[column]: Given column name in list\n",
        "\n",
        "        Args:\n",
        "            categorical_columns: A list of column names that represent categorical features.\n",
        "            numerical_columns: A list of column names that represent numerical features.\n",
        "\n",
        "        Returns: Label encoded dataset with numerical values rounded to the nearest integer.\n",
        "        \"\"\"\n",
        "        #self.data['Height'] = self.data['Height'].round(2)  # normalise the height value to x.xx\n",
        "\n",
        "        #for column in numerical_columns:\n",
        "            #self.data[column] = self.data[column].round()\n",
        "\n",
        "        for column in categorical_columns:  # label encoding to avoid high dimensionality with one-hot encoding\n",
        "            self.data[column] = self.data[column].astype(str)\n",
        "            self.data[column] = LabelEncoder().fit_transform(self.data[column])\n",
        "\n",
        "        # update feature and target variables, create separate copies for unscaled data.\n",
        "        self.X = self.data.drop('NObeyesdad', axis=1)\n",
        "        self.y = self.data['NObeyesdad']\n",
        "        self.no_X = self.data.drop('NObeyesdad', axis=1)\n",
        "        self.no_y = self.data['NObeyesdad']\n",
        "\n",
        "        print(self.data.head())  # print the encoded dataset\n",
        "\n",
        "    def feature_engineering(self):\n",
        "        \"\"\"\n",
        "        Creates a column called BMI which is the body mass index.\n",
        "\n",
        "        BMI is calculated by weight divided by height squared.\n",
        "\n",
        "          Attributes:\n",
        "            data['BMI']: Body Mass Index\n",
        "            data['Weight']: Weight in kg\n",
        "            data['Height']: Height in m\n",
        "        \"\"\"\n",
        "        self.data['BMI'] = self.data['Weight'] / (self.data['Height'] ** 2)\n",
        "\n",
        "    def split_data(self, test_size=0.2, random_state=42):\n",
        "        \"\"\"\n",
        "        Splits the data into training and testing sets.\n",
        "\n",
        "        Args:\n",
        "            test_size: The proportion of the dataset to include in the test split.\n",
        "            random_state: Controls the shuffling applied to the data before applying the split.\n",
        "        \"\"\"\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
        "            self.X, self.y, test_size=test_size, random_state=random_state\n",
        "        )\n",
        "\n",
        "    def scale_data(self):  # scale the data after splitting to prevent data leakage.\n",
        "        \"\"\"\n",
        "        Scales the data using the StandardScaler.\n",
        "        Data scaling is the process of adjusting the range of values in features to a\n",
        "        common scale.\n",
        "        \"\"\"\n",
        "        self.no_X_train = self.X_train\n",
        "        self.no_X_test = self.X_test\n",
        "        sc = StandardScaler()\n",
        "        self.X_train = sc.fit_transform(self.X_train)\n",
        "        self.X_test = sc.transform(self.X_test)\n",
        "\n",
        "    def hyperparameter_tuning(self, models):\n",
        "        \"\"\"\n",
        "        Tunes the hyperparameters of a given model using RandomSearchCV. Cross validates\n",
        "         for the detection of overfitting.\n",
        "\n",
        "        Attributes:\n",
        "          model: The name of the machine learning model to tune.\n",
        "          param_grid: A dictionary of hyperparameters and their possible values.\n",
        "\n",
        "        Args:\n",
        "          models: The list of the machine learning model to tune and train.\n",
        "\n",
        "        Returns:\n",
        "          A dictionary containing the best estimator, average CV score, accuracy score\n",
        "          and time taken to train.\n",
        "        \"\"\"\n",
        "        param_grids = {  # parameter ranges for each model\n",
        "            SVC: {'C': uniform(0.1, 10), 'kernel': ['linear', 'rbf'], 'gamma': uniform(0.0001, 10)},\n",
        "            RandomForestClassifier: {'n_estimators': randint(50, 200), 'max_depth': [None, 5, 10],\n",
        "                                     'min_samples_split': randint(2, 10)},\n",
        "            LogisticRegression: {'C': uniform(0.1, 10), 'penalty': ['l1', 'l2'], 'solver': ['liblinear', 'saga']},\n",
        "            DecisionTreeClassifier: {'max_depth': [None, 5, 10], 'min_samples_split': randint(2, 10)}\n",
        "        }\n",
        "\n",
        "        for model in models:  # trains model in param_grid\n",
        "            param_distributions = param_grids.get(type(model))  # retrieve the parameter distribution for the model\n",
        "            random_search = RandomizedSearchCV(\n",
        "                model, param_distributions, n_iter=50, cv=5, scoring='accuracy', random_state=42, n_jobs=-1\n",
        "            )\n",
        "            start_time = timer()\n",
        "            random_search.fit(self.X_train, self.y_train)\n",
        "            model_time = timer() - start_time\n",
        "            scores = cross_val_score(model, self.X, self.y,\n",
        "                                     cv=10)  # trains cross-validation scores for model comparisons for over fitting\n",
        "            average_score = scores.mean()\n",
        "            best_model = random_search.best_estimator_\n",
        "            y_pred = best_model.predict(self.X_test)\n",
        "            acc = accuracy_score(self.y_test, y_pred)\n",
        "            precision = precision_score(self.y_test, y_pred, average='weighted')\n",
        "            recall = recall_score(self.y_test, y_pred, average='weighted')\n",
        "            f1 = f1_score(self.y_test, y_pred, average='weighted')\n",
        "\n",
        "            # printing model details\n",
        "            print(f\"Model: {model.__class__.__name__}\") # gets the name of the object(model)\n",
        "            print(\"Best hyperparameters:\", random_search.best_params_)\n",
        "            print(f\"Accuracy: {acc * 100:.2f}%\\n\")\n",
        "            print(f\"Precision: {precision * 100:.2f}%\\n\")\n",
        "            print(f\"Recall: {recall * 100:.2f}%\\n\")\n",
        "            print(f\"F1-score: {f1 * 100:.2f}%\\n\")\n",
        "            print(f\"Average CV accuracy: {average_score * 100:.2f}%\\n\")\n",
        "            print(f\"Timing: {model_time:.2f}s\\n\")\n",
        "\n",
        "\n",
        "    def train(self, models):\n",
        "        \"\"\"\n",
        "        Trains the given model on the training data.\n",
        "\n",
        "        Attributes:\n",
        "        model_name: The name of the machine learning model.\n",
        "        y_pred: The predicted target variable.\n",
        "        acc: The accuracy of the model.\n",
        "        model_time: The time taken to train the model.\n",
        "\n",
        "        Args:\n",
        "            models: The list of machine learning models to train.\n",
        "\n",
        "        Returns: Scaled and unscaled accuracy scores for each model with time taken\n",
        "            to train.\n",
        "            \"\"\"\n",
        "        for model_name in models:  # trains models on unscaled data to be able to compare the differences\n",
        "            start_time = timer()\n",
        "            model_name.fit(self.no_X_train, self.y_train)\n",
        "            model_time = timer() - start_time\n",
        "            self.y_pred = model_name.predict(self.no_X_test)\n",
        "            acc = accuracy_score(self.y_test, self.y_pred)\n",
        "            print(\n",
        "                f\" Model: {model_name.__class__.__name__} Accuracy with no scaling: {acc * 100:.2f}% Timing: {model_time:.2f}s\\n\")\n",
        "\n",
        "        for model_name in models:  # trains models within a list\n",
        "            start_time = timer()\n",
        "            model_name.fit(self.X_train, self.y_train)\n",
        "            model_time = timer() - start_time\n",
        "            self.y_pred = model_name.predict(self.X_test)\n",
        "            acc = accuracy_score(self.y_test, self.y_pred)\n",
        "            print(\n",
        "                f\" Model: {model_name.__class__.__name__} Accuracy with scaling: {acc * 100:.2f}% Timing: {model_time:.2f}s\\n \")\n",
        "\n",
        "    def feature_importance(self, models):\n",
        "        for model_name in models:\n",
        "            importances = model_name.fit(self.X_train, self.y_train).feature_importances_\n",
        "            featureIm = pd.DataFrame({'Feature': self.X.columns, 'Importance': importances})\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.barh(featureIm['Feature'], featureIm['Importance'])\n",
        "            plt.xlabel('Importance')\n",
        "            plt.ylabel('Feature')\n",
        "            plt.title('Feature Importances')\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run the data science pipeline.\n",
        "    \"\"\"\n",
        "    models = [SVC(), RandomForestClassifier(), LogisticRegression(), DecisionTreeClassifier()]\n",
        "    categorical_columns = ['Gender', 'family_history_with_overweight', 'FAVC', 'CAEC', 'SMOKE', 'SCC', 'CALC', 'MTRANS',\n",
        "                           'NObeyesdad']\n",
        "    numerical_columns = ['Age', 'NCP', 'CH2O', 'FAF', 'TUE', 'BMI']\n",
        "    pipeline = DataSciencePipeline()\n",
        "    pipeline.feature_engineering()\n",
        "    pipeline.preprocess_data(categorical_columns, numerical_columns)\n",
        "    pipeline.split_data()\n",
        "    pipeline.scale_data()\n",
        "    pipeline.train(models)\n",
        "    pipeline.hyperparameter_tuning(models)\n",
        "    # pipeline.feature_importance(models = {RandomForestClassifier(), XGBClassifier()}) ignore\n",
        "\n",
        "\n",
        "main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWH_ryVfh2vR",
        "outputId": "bd3a9ae4-920e-4d53-89aa-6052aab7597f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Gender   Age  Height  Weight  family_history_with_overweight  FAVC  FCVC  \\\n",
            "0       0  21.0    1.62    64.0                               1     0   2.0   \n",
            "1       0  21.0    1.52    56.0                               1     0   3.0   \n",
            "2       1  23.0    1.80    77.0                               1     0   2.0   \n",
            "3       1  27.0    1.80    87.0                               0     0   3.0   \n",
            "4       1  22.0    1.78    89.8                               0     0   2.0   \n",
            "\n",
            "   NCP  CAEC  SMOKE  CH2O  SCC  FAF  TUE  CALC  MTRANS  NObeyesdad        BMI  \n",
            "0  3.0     2      0   2.0    0  0.0  1.0     3       3           1  24.386526  \n",
            "1  3.0     2      1   3.0    1  3.0  0.0     2       3           1  24.238227  \n",
            "2  3.0     2      0   2.0    0  2.0  1.0     1       3           1  23.765432  \n",
            "3  3.0     2      0   2.0    0  2.0  0.0     1       4           5  26.851852  \n",
            "4  1.0     2      0   2.0    0  0.0  0.0     2       3           6  28.342381  \n",
            " Model: SVC Accuracy with no scaling: 74.47% Timing: 0.54s\n",
            "\n",
            " Model: RandomForestClassifier Accuracy with no scaling: 99.05% Timing: 2.73s\n",
            "\n",
            " Model: LogisticRegression Accuracy with no scaling: 68.32% Timing: 0.55s\n",
            "\n",
            " Model: DecisionTreeClassifier Accuracy with no scaling: 96.69% Timing: 0.07s\n",
            "\n",
            " Model: SVC Accuracy with scaling: 92.20% Timing: 0.46s\n",
            " \n",
            " Model: RandomForestClassifier Accuracy with scaling: 99.29% Timing: 0.98s\n",
            " \n",
            " Model: LogisticRegression Accuracy with scaling: 90.31% Timing: 0.16s\n",
            " \n",
            " Model: DecisionTreeClassifier Accuracy with scaling: 96.45% Timing: 0.03s\n",
            " \n",
            "Model: SVC\n",
            "Best hyperparameters: {'C': 5.732755719763836, 'gamma': 6.955260864261275, 'kernel': 'linear'}\n",
            "Accuracy: 97.87%\n",
            "\n",
            "Precision: 97.88%\n",
            "\n",
            "Recall: 97.87%\n",
            "\n",
            "F1-score: 97.87%\n",
            "\n",
            "Average CV accuracy: 76.88%\n",
            "\n",
            "Timing: 62.57s\n",
            "\n",
            "Model: RandomForestClassifier\n",
            "Best hyperparameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 64}\n",
            "Accuracy: 99.29%\n",
            "\n",
            "Precision: 99.31%\n",
            "\n",
            "Recall: 99.29%\n",
            "\n",
            "F1-score: 99.29%\n",
            "\n",
            "Average CV accuracy: 98.86%\n",
            "\n",
            "Timing: 93.68s\n",
            "\n",
            "Model: LogisticRegression\n",
            "Best hyperparameters: {'C': 0.984925020519195, 'penalty': 'l1', 'solver': 'saga'}\n",
            "Accuracy: 94.33%\n",
            "\n",
            "Precision: 94.53%\n",
            "\n",
            "Recall: 94.33%\n",
            "\n",
            "F1-score: 94.25%\n",
            "\n",
            "Average CV accuracy: 68.22%\n",
            "\n",
            "Timing: 54.89s\n",
            "\n",
            "Model: DecisionTreeClassifier\n",
            "Best hyperparameters: {'max_depth': None, 'min_samples_split': 2}\n",
            "Accuracy: 96.45%\n",
            "\n",
            "Precision: 96.57%\n",
            "\n",
            "Recall: 96.45%\n",
            "\n",
            "F1-score: 96.46%\n",
            "\n",
            "Average CV accuracy: 97.21%\n",
            "\n",
            "Timing: 1.75s\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
